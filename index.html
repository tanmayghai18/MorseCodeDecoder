<!DOCTYPE HTML>
<html xmlns="http://www.w3.org/1999/html">
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Deciphering Morse Code Audio from Various Sound Sources</title>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <!-- Favicon -->
      <link rel="shortcut icon" href="./images/logo.png">
      <!-- Fonts -->
      <link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
      <!-- Animate.css -->
      <link rel="stylesheet" href="./css/animate.css">
      <!-- Icomoon Icon Fonts-->
      <link rel="stylesheet" href="./css/icomoon.css">
      <!-- Bootstrap  -->
      <link rel="stylesheet" href="./css/bootstrap.css">
      <!-- Theme style  -->
      <link rel="stylesheet" href="./css/style.css">
      <!-- Blog stuff -->
      <link rel="stylesheet" href="./css/syntax.css">
      <script type="text/javascript" async
         src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <!-- End blog stuff -->
      <!-- Modernizr JS -->
      <script src="../js/modernizr-2.6.2.min.js"></script>
      <!-- FOR IE9 below -->
      <!--[if lt IE 9]>
      <script src="../js/respond.min.js"></script>
      <![endif]-->
   </head>
   <body>
      <div id="colorlib-page">
      <div id="colorlib-main">
      <div id="colorlib-post">
      <h1 class="post-title" style="text-align: center">
         Deciphering Morse Code Audio from Various Sound Sources
      </h1>
      <h3 style="text-align: center">         
         CSCI 566: Deep Learning, Fall 2020
      </h3>
      <h4 style="text-align: center">         
         TAROS (team id: 9)
      </h4>
      <p><img src="model.gif" alt="" style="width: 80%; display: block; margin-left: auto; margin-right: auto;" /></p>
      <h4 style="text-align: center">         
         <a href="demo.html">Live Demo</a> | <a href="https://github.com/OnurOrhan/taros-deep-learning">Source Code</a> | <a href="https://drive.google.com/file/d/13p0bNzOEm7Hwt-gDemuwvl4lOxGW5G3V/view?usp=sharing">Project Video</a>
      </h4>
      <h2 id="abstract">Abstract</h2>
      <p><a href="https://en.wikipedia.org/wiki/Morse_code">Morse code</a> is a series of dots and dashes representing different text characters, digits and punctuation marks. It is transmitted by on-off keying of an information carrying medium such as electric current, radio waves, visible light or sound waves. In audio form, morse code can be generated via various methods, like tapping (on different materials such as bells, musical instruments, car honks, etc.). Humans can decipher morse code in any given sound source, if they are first trained on a single source. This project's goal is to develop a system which will replicate this "human-like" behavior in deciphering morse code, i.e. the system will be able to decipher the morse code into english text regardless of the sound source used to generate the input.</p>
      <h3 id="">Problem Statement</h3>
      <p> More specifically, to develop such a morse code recognition system, we will decipher morse code audio into english text, irrespective of the sound source used to generate the morse code</p>
         <p> <b>Input</b>: morse code audio .wav</p>
         <p> <b>Output</b>: english text corresponding to the inputted morse </p>
      <h2 id="approach">Approach</h3>
         <p>Our approach is broken up into two main phases: dataset curation and training. In dataset curation, we pre-recorded audio files of different mediums and along with a text corpus, created a labeled morse code dataset. For training, we trained a C-RNN model with 2 Bi-directional LSTMs and CTC loss.</p>
      <h3 id="dataset curation">Dataset Curation</h3>
      <p> We used 13 different audio mediums for testing and experimenting throughout our project; you can hear small snippets of 12 of them below (the 13th medium was "beeps" which are regularly used with morse code) </p>
         
      <audio controls>
         <source src="bell.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="clap.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="drums_2.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="glass.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="guitar1.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="guitar2.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="guitar3.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="guitar4.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="guitar5.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="guitar6.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="horn.wav" type="audio/wav">
      </audio>
      <audio controls>
         <source src="steel.wav" type="audio/wav">
      </audio>
      
      <br>
         
         <p></p>
         
      <p> Our first step to formulate our dataset was to take each of these audio waves and create individual "dots" and "dashes" out of them. Below is an example (with the guitar1 medium): </p>
      
      <table style="width:100%">
        <tr>
          <th>dot (100 ms)</th>
          <th>dash (300 ms)</th>
        </tr>
      </table>
      
      <audio controls>
         <source src="guitar1-dot.ogg" type="audio/ogg">
      </audio>
      <audio controls>
         <source src="guitar1-dash.ogg" type="audio/ogg">
      </audio>
         
      <p>After this, with the help of our <a href="https://www.kaggle.com/mikeortman/wikipedia-sentences">text corpus</a>, we formulated sentences using the above dot and dash files. Here's <b>"In the end, we all felt like we ate too much"</b> in morse code!</p>
         
      <audio controls>
         <source src="6_clap.wav" type="audio/wav">
      </audio>
         
         <p> </p>
         
         <h4 id="">Artificially Generated Noise</h3>
         <p>To replicate a real-world use case, we also created a separate dataset that included 3 types of temporal randomness:</p>
            <ul>
               <li><b>Dot/Dash Time Units</b>: In our original dataset, we had dots represent 1 time unit, and dashes represent 3 time units. Here, we varied the time for each dot and dash via some uniform randomness within a small range.</li>
              <li><b>Time Gaps</b>: Additionally, we added some noise to the gaps in time between each character.</li>
              <li><b>Amplitude of Dot/Dash</b>: Finally, we added temporal variation in how "intense" the sound was for the various dots and dashes.</li>
            </ul>
            
      
      <br>
      
         <p></p>
         
      <h3 id="training">Training</h3>
       
         <p> Our best performing model ended up being a C-RNN architecture with <a href="https://distill.pub/2017/ctc/">CTC loss</a> (Connectionist Temporal Classification), where we first converted our morse audio files into 2d images representing the amplitude vs. time of the wave. However, this took experimentation and iteration to realize this was our best approach. Below, we detail two of our earlier approaches before arriving at our final model.</p>
         
         <h4 id="">Iteration 1: CNN - Linear - Bi-directional GRU (Gated Recurrent Unit)</h4>
         
         <h4 id="">Iteration 2: CNN - LSTM - CCE (Categorical Cross Entropy)</h4>
         
         <p> This model utilizes a CNN - LSTM architecture with 1-D input (being the audio arrays itself), followed by a loss calculation done by <a href="tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy">categorical cross entropy</a> (or softmax loss). Originally, we had used <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError">MSE</a> (or mean-squared error) loss to evaluate this model, however, after some research, we realized that CCE (for our use case) was a much better measure of classification, due to the fact that we have over 50+ classes for our task. In such tasks, as compared to regression, MSE doesn't penalize mis-classifications enough to learn effectively. After over 20+ different experiments, tuning and hyperparameter tuning for this model, we settled on 100 epochs, with a batch size of 16. Our highest run for this model was <b>~70.28%</b>. Below you can see the training and validation loss curves as well as some sample reconstructions with this method.</p>
         
         <p> Architecturally, we used two 1-D convolution layers, the first with a 16 filter size and the second with 32, respectively, both with kernel size of 3. The first convolution layer is following by a (15, 15) max pooling layer and the second convolution layer is followed by a batch normalization layer and a (12, 12) max pooling layer. Following all this, we have 2 bi-directional LSTM layers, each with 256 units, followed by a dense-softmax activation layer. We settled on an Adam optimizer with a learning rate of 0.0005. </p>
         
         <div class="carousel-non-resize">
           <div class="carousel-col2">
             <img src="images/cce_loss_experiment.png" style="width:100%" />
           </div>  
           <div class="carousel-col2">
             <img src="images/cce_loss_reconstructions.png" style="width:100%" />
           </div>
         </div>
         
         <h4 id="">Final Model: CNN - LSTM - CTC (Connectionist Temporal Classification)</h4>
         
         
      <h2 id="experiments">Experiments</h2>
         
         <h3 id="">Comparison with Baseline (held one out)</h3>

            <p> The proposed design and choice of dataset outperformed current design and dataset to recognize morse code from multitude sources of mediums. Our proposed model is trained on 11 different mediums and tested on an unknown medium. We got an character level accuracy of <b>97%</b>. The existing models in which the dataset was trained only on beeps, when tested on a unseen mediums performed with an character level accuracy of 2 percent. A simple reason for this is when a single medium is given for training, model can easily memorize and overfit the model but at testing time, when something different is encountered, the model fails. Below you can see the loss curves of our proposed model vs. the existing model. Additionally, below that, you can see how our model performed across the various different sound sources. Performance was consistent across almost all, except for <b>horn</b> which might have been due to a lack of other synthetic sound sources being a part of the dataset. </p>
            
            <table style="width:100%">
               <tr>
                  <th>Proposed Model</th>
                  <th>Existing Model</th>
               </tr>
            </table>
         
         <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/proposed_model.png" style="width:100%" />
               </div>
               <div class="carousel-col2">
                  <img src="images/existing_model.png" style="width:100%" />
               </div>
         </div>
         
         <table style="width:100%">
               <tr>
                  <th>Training Loss (all sounds)</th>
                  <th>Validation Loss (all sounds)</th>
               </tr>
            </table>
         
         <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/all_sounds1.png" style="width:100%" />
               </div>
               <div class="carousel-col2">
                  <img src="images/all_sounds2.png" style="width:100%" />
               </div>
         </div>
         
         
         <h3 id="">Holding Out Mediums (3-6-9)</h3>
            <p>Upon evaluation, it was observed that performance of the proposed model depends on the number of sounds sources used to train it.
               When the model was trained on 3 sound sources, the character accuracy <b>41.3%</b> which increased to <b>48.1%</b> with use of 6 mediums and to <b>76.13%</b> with 9 mediums. As we can see from the plots below, apart from having better accuracy with increasing mediums, we also have faster convergence per epoch. <b>These results clearly indicated the benefits of having more mediums</b>.</p>

            <table style="width:100%">
               <tr>
                  <th>3 mediums</th>
                  <th>6 mediums</th>
                  <th>9 mediums</th>
               </tr>
            </table>
         
            <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/3_mediums.png" style="width:100%" />
               </div>
               <div class="carousel-col2">
                  <img src="images/6_mediums.png" style="width:100%" />
               </div>
               <div class="carousel-col2">
                  <img src="images/9_mediums.png" style="width:100%" />
               </div>
            </div>
         <h3 id="">Experimenting with Artificial Temporal Randomness</h3>
            <p> Given a human generated morse audio file, it is clear that duration of dots and dash and the timing between them can be a bit off unlike a computer generated morse code. This variabiliy is a must to capture for a robust Morse Code Recognition model. For that purpose, random temporal noise was introduced between dots and dashes and in the length of dots and dashes. The above model was trained with some fine tuning. We were able to achieve an accuracy of <b>98.37%</b> on a test set of unseen mediums with temporal variability. Below is the loss curve we observed while training with randomness as well as some reconstructions. <b>As you can see, our model still did really well amidst the temporal and spacial variation.</b> </p>
         
            <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/randomness.png" style="width:100%" />
               </div>
               <div class="carousel-col2">
                  <img src="images/randomness_reconstruction.png" style="width:100%" />
               </div>
            </div>
         
         <h3 id="">Mixing Mediums for each Input</h3>
            <p>  We know that when morse code is being generated by human, the amplitude of each dot and dash for each character may differ. Example - one may knock lightly first time and stronly second time. This can be called as Spacial Noise. It is being called spacial noise because different level of amplitudes may suggest different character but in reality, it can be the same character. To tackle this problem, we used multiple medias to create single audio file. <b>Here's a mixed-medium input audio file: </b>.
            </p>

            <audio controls>
               <source src="mixed.ogg" type="audio/ogg">
            </audio>
         <br>
            <p> A dataset of audio files similar to above was used to train the model. Unfortunately, the model didnt perform well. One reason can be that the variation of amplitudes in each audio file was too synthesised and hence model was unable to effectively capture the sequence of characters. </p>

            <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/mixed.png" style="width:50%" />
               </div>

            </div>
         
      <h2 id="future work">Future and Related Work</h2>
         
         
   </body>
</html>
