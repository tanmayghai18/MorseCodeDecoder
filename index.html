<!DOCTYPE HTML>
<html xmlns="http://www.w3.org/1999/html">
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Deciphering Morse Code Audio from Various Sound Sources</title>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <!-- Favicon -->
      <link rel="shortcut icon" href="./images/logo.png">
      <!-- Fonts -->
      <link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
      <!-- Animate.css -->
      <link rel="stylesheet" href="./css/animate.css">
      <!-- Icomoon Icon Fonts-->
      <link rel="stylesheet" href="./css/icomoon.css">
      <!-- Bootstrap  -->
      <link rel="stylesheet" href="./css/bootstrap.css">
      <!-- Theme style  -->
      <link rel="stylesheet" href="./css/style.css">
      <!-- Blog stuff -->
      <link rel="stylesheet" href="./css/syntax.css">
       <link rel="stylesheet" href="./custom_styles.css">
      <script type="text/javascript" async
         src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <!-- End blog stuff -->
      <!-- Modernizr JS -->
      <script src="../js/modernizr-2.6.2.min.js"></script>
      <!-- FOR IE9 below -->
      <!--[if lt IE 9]>
      <script src="../js/respond.min.js"></script>
      <![endif]-->
   </head>
   <body>
      <div id="colorlib-page">
      <div id="colorlib-main">
      <div id="colorlib-post">
      <h1 class="post-title" style="text-align: center">
         Deciphering Morse Code Audio from Various Sound Sources
      </h1>
      <h3 style="text-align: center">         
         CSCI 566: Deep Learning, Fall 2020
      </h3>
      <h4 style="text-align: center">         
         TAROS (team id: 9)
      </h4>
      <p><img src="model.gif" alt="" style="width: 80%; display: block; margin-left: auto; margin-right: auto;" /></p>
      <h4 style="text-align: center">         
         <a href="demo.html">Live Demo</a> | <a href="https://github.com/OnurOrhan/taros-deep-learning">Source Code</a> | <a href="https://drive.google.com/file/d/13p0bNzOEm7Hwt-gDemuwvl4lOxGW5G3V/view?usp=sharing">Project Video</a>
      </h4>
      <h2 id="abstract">Abstract</h2>
      <p><a href="https://en.wikipedia.org/wiki/Morse_code">Morse code</a> is a series of dots and dashes representing different text characters, digits and punctuation marks. It is transmitted by on-off keying of an information carrying medium such as electric current, radio waves, visible light or sound waves. In audio form, morse code can be generated via various methods, like tapping (on different materials such as bells, musical instruments, car honks, etc.). Humans can decipher morse code in any given sound source, if they are first trained on a single source. This project's goal is to develop a system which will replicate this "human-like" behavior in deciphering morse code, i.e. the system will be able to decipher the morse code into english text regardless of the sound source used to generate the input.</p>
      <h2 id="problem_statement">Problem Statement</h2>
      <p> More specifically, to develop such a morse code recognition system, we will decipher morse code audio into english text, irrespective of the sound source used to generate the morse code</p>
         <p> <b>Input</b>: morse code audio .wav</p>
         <p> <b>Output</b>: english text corresponding to the inputted morse </p>
      <h2 id="approach">Approach</h2>
         <p>
            Our approach is broken up into two main phases: dataset curation and training. In dataset curation, we pre-recorded audio files of different mediums and along with a text corpus, created a labeled morse code dataset. For training, we trained a C-RNN model with 2 Bi-directional LSTMs and CTC loss.
         </p>
      <h3 id="dataset curation">Dataset Curation</h3>
      <p>
         To the best of our knowledge there is no publicly available dataset that suits the aim of our problem. Thus we have created a dataset by pre-recording sounds from 13 different sound sources and further generated morse code audio files from them. We used this dataset for testing and experimenting throughout this project.
      </p>
      <p>
         As a first step we have recorded the sounds from 13 different sound sources and preprocessed them to remove leading and trailing silence; you can hear small snippets of 12 of them below (the 13th medium was "beeps" which are regularly used with morse code)
      </p>
      <table class="audio_files" style="width:100%">
         <tr>
            <td><span class="audio_name">Bell</span></td>
            <td><audio controls>
               <source src="bell.wav" type="audio/wav">
            </audio></td>
            <td><span class="audio_name"> Clap </span>
            <td><audio controls>
               <source src="clap.wav" type="audio/wav">
            </audio></td>
         </tr>
         <tr>
            <td style = "border: 1px solid white" ><span class="audio_name"> Drums </span>
            <td><audio controls>
               <source src="drums_2.wav" type="audio/wav">
            </audio></td>
            <td>
               <span class="audio_name"> Glass </span></td>
            <td><audio controls>
               <source src="glass.wav" type="audio/wav">
            </audio>
            </td>
         </tr>
         <tr>
            <td>
               <span class="audio_name"> Guitar 1 </span></td>
            <td>
               <audio controls>
               <source src="guitar1.wav" type="audio/wav">
            </audio>
            </td>
            <td>
               <span class="audio_name"> Guitar 2 </span></td>
            <td>
            <audio controls>
               <source src="guitar2.wav" type="audio/wav">
            </audio>
            </td>
         </tr>
         <tr>
            <td>
               <span class="audio_name"> Guitar 3 </span></td>
            <td>
            <audio controls>
               <source src="guitar3.wav" type="audio/wav">
            </audio>
            </td>
            <td>
               <span class="audio_name"> Guitar 4 </span></td>
            <td>
            <audio controls>
               <source src="guitar4.wav" type="audio/wav">
            </audio>
            </td>
         </tr>
         <tr>
            <td>
            <span class="audio_name"> Guitar 5 </span>
            </td>
            <td>
            <audio controls>
               <source src="guitar5.wav" type="audio/wav">
            </audio>
            </td>
            <td>
            <span class="audio_name"> Guitar 6 </span>
            </td>
            <td>
               <audio controls>
               <source src="guitar6.wav" type="audio/wav">
            </audio>
            </td>
         </tr>
         <tr>
            <td>
            <span class="audio_name"> Horn </span>
            </td>
            <td>
            <audio controls>
               <source src="horn.wav" type="audio/wav">
            </audio>
            </td>
            <td>
            <span class="audio_name"> Steel </span>
            </td>
            <td>
               <audio controls>
               <source src="steel.wav" type="audio/wav">
            </audio>
            </td>
         </tr>
      </table>

      <p> Our Second step to formulate our dataset was to take each of these audio waves and create individual "dots" and "dashes" out of them. Below is an example (with the guitar1 medium): </p>

      <table class= "audio_files" style="width:auto">
         <tr>
            <td>
               <span> Dot(100ms) </span>
            </td>
            <td>
               <audio controls>
                  <source src="guitar1-dot.ogg" type="audio/ogg">
               </audio>
            </td>
         </tr>
         <tr>
            <td>
                <span> Dash (300ms)  </span>
            </td>
            <td>
               <audio controls>
                   <source src="guitar1-dash.ogg" type="audio/ogg">
                </audio>
            </td>
         </tr>
      </table>

<!--      <div>-->
<!--         <p>-->
<!--            Using these dot and dash audio files and <a href="">Morse Code Nomenclature</a>, we have further generated character audio files. Below is a sample for character (with medium Drums):-->
<!--         </p>-->
<!--         <p>-->
<!--         <audio controls>-->
<!--            <source src="A.ogg" type="audio/ogg">-->
<!--         </audio>-->
<!--         </p>-->
<!--      </div>-->

      <div>
         <p>
            After this, with the help of our <a href="https://www.kaggle.com/mikeortman/wikipedia-sentences">text corpus</a> and <a href="">Morse Code Nomenclature</a>, we formulated morse characters and then sentences, using the above dot and dash files. Here's <b>"In the end, we all felt like we ate too much"</b> in morse code (with medium clap)!
         </p>
         <p>
         <audio controls>
            <source src="6_clap.wav" type="audio/wav">
         </audio>
         </p>
      </div>

      <div>
         <p>
            <h4>Artificially Generated Noise</h4>
            To replicate a real-world use case, we also created a separate dataset that included 3 types of temporal randomness:
            <ul>
               <li><b>Dot/Dash Time Units</b>: In our original dataset, we had dots represent 1 time unit, and dashes represent 3 time units. Here, we varied the time for each dot and dash via some uniform randomness within a small range.</li>
               <li><b>Time Gaps</b>: Additionally, we added some noise to the gaps in time between each character.</li>
               <li><b>Amplitude of Dot/Dash</b>: Finally, we added temporal variation in how "intense" the sound was for the various dots and dashes.</li>
            </ul>
         </p>
      </div>

         
      <h3 id="training">Training</h3>
       
         <p> Our best performing model ended up being a C-RNN architecture with <a href="https://distill.pub/2017/ctc/">CTC loss</a> (Connectionist Temporal Classification), where we first converted our morse audio files into 2d images representing the amplitude vs. time of the wave. However, this took experimentation and iteration to realize this was our best approach. Below, we detail two of our earlier approaches before arriving at our final model.</p>

         <div>
            <h4>Iteration 1: CNN - Linear - Bi-directional GRU (Gated Recurrent Unit)</h4>
            <p>
               Inspired by deep speech 2 architecture which has given good results in the field of speech recognition, we have constructed a model that takes 1D arrays of audio samples as input and generates deciphered text. This model consists of 4 1D convolution layers with filter sizes 8, 16, 32, 64, followed by a linear layer, two bidirectional GRUs with 256 hidden units and a linear layer with 70 hidden units and a dense layer with softmax activation. After each convolution we are using a dropout layer with a rate of 0.3. The loss is calculated using categorical cross entropy and we used adam optimizer with a learning rate of 0.001.
            </p>
            <p>
               After multiple trails by tuning the parameters like number of layers, hidden units we are able to achieve a training accuracy of only 60% and our validation accuracy kept oscillating between 20% - 50%. For our test data we have received an accuracy of 53%. After several trails we came to a conclusion that this architecture will not give promising results.
            </p>
         </div>
         <div>
            <h4>Iteration 2: CNN - LSTM - CCE (Categorical Cross Entropy)</h4>
         
            <p> This model utilizes a CNN - LSTM architecture with 1-D input (being the audio arrays itself), followed by a loss calculation done by <a href="tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy">categorical cross entropy</a> (or softmax loss). Originally, we had used <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError">MSE</a> (or mean-squared error) loss to evaluate this model, however, after some research, we realized that CCE (for our use case) was a much better measure of classification, due to the fact that we have over 50+ classes for our task. In such tasks, as compared to regression, MSE doesn't penalize mis-classifications enough to learn effectively. After over 20+ different experiments, tuning and hyperparameter tuning for this model, we settled on 100 epochs, with a batch size of 16. Our highest run for this model was <b>~70.28%</b>. Below you can see the training and validation loss curves as well as some sample reconstructions with this method.</p>

            <p> Architecturally, we used two 1-D convolution layers, the first with a 16 filter size and the second with 32, respectively, both with kernel size of 3. The first convolution layer is following by a (15, 15) max pooling layer and the second convolution layer is followed by a batch normalization layer and a (12, 12) max pooling layer. Following all this, we have 2 bi-directional LSTM layers, each with 256 units, followed by a dense-softmax activation layer. We settled on an Adam optimizer with a learning rate of 0.0005. </p>

            <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/cce_loss_experiment.png" style="width:100%" />
               </div>
               <div class="carousel-col2">
                  <img src="images/cce_loss_reconstructions.png" style="width:100%" />
               </div>
            </div>
         </div>

         <h4 id="">Final Model: CNN - LSTM - CTC (Connectionist Temporal Classification)</h4>
         

      <h2 id="experiments">Experiments</h2>

         <h3>Comparison with Baseline (held one out)</h3>

         <p> The proposed design and choice of dataset outperformed current design and dataset to recognize morse code from multitude sources of mediums. Our proposed model is trained on 11 different mediums and tested on an unknown medium. We got an character level accuracy of <b>97%</b>. The existing models in which the dataset was trained only on beeps, when tested on a unseen mediums performed with an character level accuracy of 2 percent. A simple reason for this is when a single medium is given for training, model can easily memorize and overfit the model but at testing time, when something different is encountered, the model fails. Below you can see the loss curves of our proposed model vs. the existing model.</p>
            <table class="evaluation_images" style="width:100%">
               <tr>
                  <td><img src="images/proposed_model.png" style="width:100%" /></td>
                  <td><img src="images/proposed_model.png" style="width:100%" /></td>
               </tr>
                <tr>
                  <th>Proposed Model</th>
                  <th>Existing Model</th>
               </tr>
            </table>

         <p> Additionally, below, you can see how our model performed across the various different sound sources. Performance was consistent across almost all, except for <b>horn</b> which might have been due to a lack of other synthetic sound sources being a part of the dataset. </p>

          <table class="evaluation_images" style="width:100%">
               <tr>
                  <td>
                       <img src="images/all_sounds1.png" style="width:100%" />
                  </td>
                  <td>
                      <img src="images/all_sounds2.png" style="width:100%" />
                  </td>
               </tr>
                <tr>
                  <th>Training Loss (all sounds)</th>
                  <th>Validation Loss (all sounds)</th>
               </tr>
          </table>
<!--         <div class="carousel-non-resize">-->
<!--               <div class="carousel-col2">-->
<!--                  <img src="images/proposed_model.png" style="width:100%" />-->
<!--               </div>-->
<!--               <div class="carousel-col2">-->
<!--                  <img src="images/existing_model.png" style="width:100%" />-->
<!--               </div>-->
<!--         </div>-->

<!--         <table style="width:100%">-->
<!--               <tr>-->
<!--                  <th>Training Loss (all sounds)</th>-->
<!--                  <th>Validation Loss (all sounds)</th>-->
<!--               </tr>-->
<!--            </table>-->

<!--         <div class="carousel-non-resize">-->
<!--               <div class="carousel-col2">-->
<!--                  <img src="images/all_sounds1.png" style="width:100%" />-->
<!--               </div>-->
<!--               <div class="carousel-col2">-->
<!--                  <img src="images/all_sounds2.png" style="width:100%" />-->
<!--               </div>-->
<!--         </div>-->


         <h3>Holding Out Mediums (3-6-9)</h3>
            <p>Upon evaluation, it was observed that performance of the proposed model depends on the number of sounds sources used to train it.
               When the model was trained on 3 sound sources, the character accuracy <b>41.3%</b> which increased to <b>48.1%</b> with use of 6 mediums and to <b>76.13%</b> with 9 mediums. As we can see from the plots below, apart from having better accuracy with increasing mediums, we also have faster convergence per epoch. <b>These results clearly indicated the benefits of having more mediums</b>.</p>

            <table class="evaluation_images" style="width:100%">
               <tr>
                  <td><img src="images/3_mediums.png" style="width:100%" /></td>
                  <td><img src="images/6_mediums.png" style="width:100%" /></td>
                  <td><img src="images/9_mediums.png" style="width:100%" /></td>
               </tr>
               <tr>
                  <th>3 mediums</th>
                  <th>6 mediums</th>
                  <th>9 mediums</th>
               </tr>
            </table>

         <h3>Experimenting with Artificial Temporal Randomness</h3>
            <p> Given a human generated morse audio file, it is clear that duration of dots and dash and the timing between them can be a bit off unlike a computer generated morse code. This variabiliy is a must to capture for a robust Morse Code Recognition model. For that purpose, random temporal noise was introduced between dots and dashes and in the length of dots and dashes. The above model was trained with some fine tuning. We were able to achieve an accuracy of <b>98.37%</b> on a test set of unseen mediums with temporal variability. Below is the loss curve we observed while training with randomness as well as some reconstructions. <b>As you can see, our model still did really well amidst the temporal and spacial variation.</b> </p>

            <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/randomness.png" style="width:100%" />
               </div>
               <div class="carousel-col2">
                  <img src="images/randomness_reconstruction.png" style="width:100%" />
               </div>
            </div>

         <h3>Comparision between Amplitude-time graph and Spectrograms</h3>
         <p>The proposed design which constructs amplitude time graphs from the audio files out-performs most existing approaches that make use of frequency spectrograms. The model when trained with amplitude-time graphs on 11 different sound sources and tested with an unseen sound source 'Clap' gave a character level accuracy of <b>97%</b>. Similarly, the same model when trained with frequency spectrograms on same training and test sets gave a character level accuracy of 74%. Also, as you can see in the graph below when trained with Amplitude-time graph the model converges faster than frequency spectrograms. <b>This suggests that amplitude time graphs work well in generalised morse code recognition than frequency spectrograms. </b></p>

            <table class="evaluation_images" style="width:100%">
               <tr>
                  <td><img src="./images/amplitude_time.png" style="width:100%; height: 100%;" /></td>
                  <td><img src="./images/spectrogram_graph.png" style="width:100%; height: 100%;" /></td>
               </tr>
               <tr>
                  <th>Amplitude-time graph</th>
                  <th>Frequency Spectrogram</th>
               </tr>
            </table>

         <h3>Mixing Mediums for each Input</h3>
            <p>  We know that when morse code is being generated by human, the amplitude of each dot and dash for each character may differ. Example - one may knock lightly first time and stronly second time. This can be called as Spacial Noise. It is being called spacial noise because different level of amplitudes may suggest different character but in reality, it can be the same character. To tackle this problem, we used multiple medias to create single audio file. <b>Here's a mixed-medium input audio file: </b>.
            </p>

            <audio controls>
               <source src="mixed.ogg" type="audio/ogg">
            </audio>
         <br>
            <p> A dataset of audio files similar to above was used to train the model. Unfortunately, the model didnt perform well. One reason can be that the variation of amplitudes in each audio file was too synthesised and hence model was unable to effectively capture the sequence of characters. </p>

            <div class="carousel-non-resize">
               <div class="carousel-col2">
                  <img src="images/mixed.png" style="width:50%" />
               </div>

            </div>

      <div>
         <h2 id="future work">Next Steps</h2>
         <p>Our results provide insight into the challenges underlying in deciphering morse code audio files. We suggest the following for future work:</p>
         <ul>
            <li>Identify the morse code audio from a single sound source provided there are multiple sounds(noise) in the audio file.</li>
            <li>Add synthesized sound mediums to the dataset which helps in increasing the character level accuray when tested on mediums like horn.</li>
            <li>Addition of mediums with larger amplitude decay rate (e.g. knocking on a table)</li>
            <li>First train on artificially synthesized dataset and then retrain a few layers with a small dataset of human recorded morse audio files on various mediums.</li>
         </ul>
      </div>
      </div>
      </div>
      </div>


   </body>
</html>
